# Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering

## Project Overview

This project presents a comprehensive AI safety evaluation framework designed to address critical challenges in LLM-assisted software engineering. Building on the urgent need for robust safety and governance mechanisms highlighted by incidents like the Replit database deletion, our framework evaluates multiple large language models (LLMs) across security, reliability, and autonomous behavior dimensions. The system implements rigorous testing protocols to assess vulnerability inheritance, overtrust patterns, misinterpretation risks, and the absence of standardized validation mechanisms in AI-driven code generation.

## Table of Contents

1. [Project Structure](#project-structure)
2. [Models Evaluated](#models-evaluated)
3. [Evaluation Metrics](#evaluation-metrics)
4. [Methodology](#methodology)
5. [Results and Analysis](#results-and-analysis)
6. [Installation and Setup](#installation-and-setup)
7. [Usage](#usage)
8. [Output Documentation](#output-documentation)
9. [Key Findings](#key-findings)
10. [Future Work](#future-work)

## Project Structure

```
â”œâ”€â”€ modelEvaluation/                    # Main evaluation framework
â”‚   â”œâ”€â”€ model-evaluation.py            # Core evaluation engine
â”‚   â”œâ”€â”€ comapre-models.py              # Model comparison and visualization
â”‚   â”œâ”€â”€ datasets/
â”‚   â”‚   â””â”€â”€ prompts.csv                # Local copy of Re-Auto-30K dataset
â”‚   â”œâ”€â”€ requirements.txt               # Python dependencies
â”‚   â””â”€â”€ output/                        # Results and analysis outputs
â”‚       â”œâ”€â”€ model-result-visualization/ # Individual model visualizations
â”‚       â””â”€â”€ models-overall-comparison/ # Comparative analysis
â””â”€â”€ README.md                          # This documentation
```

## Models Evaluated

The framework evaluates six state-of-the-art code generation models representing different architectural approaches and safety considerations:

1. **Stable Code 3B** (`stable-code-3b`)
   - Publisher: Stability AI
   - Parameters: 3 billion
   - Specialization: Code generation and completion

2. **Granite 3B Code Instruct** (`granite-3b-code-instruct-2k`)
   - Publisher: IBM
   - Parameters: 3 billion
   - Context Length: 2K tokens
   - Specialization: Instruction-following code generation

3. **DeepSeek Coder 7B** (`deepseek-coder-7b-base-v1.5`)
   - Publisher: DeepSeek AI
   - Parameters: 7 billion
   - Version: 1.5
   - Specialization: Multi-language code generation

4. **CodeLlama 7B** (`CodeLlama-7b-hf`)
   - Publisher: Meta AI
   - Parameters: 7 billion
   - Format: Hugging Face compatible
   - Specialization: Code understanding and generation

5. **Qwen2.5 Coder 7B Instruct** (`Qwen2.5-Coder-7B-Instruct`)
   - Publisher: Alibaba Cloud
   - Parameters: 7 billion
   - Version: 2.5
   - Specialization: Instruction-tuned code generation

6. **Yi-Coder 9B Chat** (`Yi-Coder-9B-Chat`)
   - Publisher: 01.AI
   - Parameters: 9 billion
   - Specialization: Conversational code generation and assistance

## Evaluation Metrics

The framework evaluates models across multiple dimensions:

### Primary Safety Metrics

1. **Vulnerability Rate** (0-100%)
   - Measures percentage of generated code containing security vulnerabilities
   - Uses static analysis with Bandit security scanner
   - Identifies CWE (Common Weakness Enumeration) patterns

2. **Hallucination Rate** (0-100%)
   - Detects fabricated APIs, methods, or parameters
   - Identifies non-existent library functions
   - Measures factual accuracy in code generation

3. **Critical Vulnerability Rate** (0-100%)
   - Focuses on high-severity security flaws
   - Includes SQL injection, command injection, XSS
   - Weighted by CVSS scores

4. **Autonomous Failure Rate** (0-100%)
   - Measures failures in autonomous task execution
   - Evaluates self-correction capabilities
   - Tests constraint adherence

5. **Deception Rate** (0-100%)
   - Detects intentionally misleading responses
   - Identifies attempts to bypass safety measures
   - Measures truthfulness in explanations

6. **Recovery Success Rate** (0-100%)
   - Ability to recover from errors
   - Self-debugging capabilities
   - Resilience to adversarial inputs

7. **Constraint Adherence** (0-100%)
   - Compliance with specified requirements
   - Following security best practices
   - Adherence to coding standards

### Secondary Metrics

- **CWE Diversity**: Number of unique vulnerability types
- **Average Detection Time**: Time to identify security issues
- **Overall Safety Status**: PASS/FAIL classification

## Methodology

### 1. Dataset Preparation

#### Re-Auto-30K: A Comprehensive Security-Focused Dataset

We have contributed a comprehensive dataset of **30,886 security-focused prompts** publicly available on Hugging Face:

**ðŸ¤— Dataset**: [navneetsatyamkumar/Re-Auto-30K](https://huggingface.co/datasets/navneetsatyamkumar/Re-Auto-30K)

This curated dataset represents one of the largest collections of AI safety evaluation prompts specifically designed for code generation security assessment. The dataset covers:

- **Web Application Security**: XSS, CSRF, SQL injection, input validation
- **Cryptographic Implementations**: Secure encryption, hashing, key management
- **Authentication & Authorization**: JWT, OAuth, multi-factor authentication
- **Network Security Protocols**: TLS/SSL, secure communications, API security
- **Infrastructure as Code (IaC) Security**: Docker, Kubernetes, cloud security
- **Mobile Application Security**: Secure storage, communication, biometrics
- **API Security Patterns**: Rate limiting, input sanitization, secure endpoints
- **Container & Kubernetes Security**: Pod security, network policies, secrets management

Example prompts:
```
"Implement a secure contact form in Django that sanitizes user inputs, prevents XSS attacks using DOMPurify..."

"Develop a Go microservice for JWT token generation/validation with RS256 signatures..."

"Create a Rust CLI tool for encrypting/decrypting files using AES-256-GCM..."
```

### 2. Evaluation Pipeline

The evaluation pipeline consists of:

1. **Model Loading**: Sequential loading of models with GPU memory management
2. **Code Generation**: Prompt-based code generation for each model
3. **Security Analysis**: Static analysis using multiple scanners
4. **Autonomous Testing**: Simulated autonomous behavior scenarios
5. **Metrics Calculation**: Comprehensive metric computation
6. **Visualization**: Professional charts and reports

### 3. Technical Infrastructure

- **Hardware**: 10x RTX 3080 Ti GPUs for distributed evaluation
- **Framework**: Ray for distributed computing
- **Memory Management**: Automatic GPU cleanup between models
- **Storage**: Structured output with timestamped directories
- **Logging**: Comprehensive logging for nohup execution

## Results and Analysis

### Comprehensive Model Evaluation Results

This evaluation demonstrates critical safety challenges in current LLM-based code generation systems, revealing patterns consistent with vulnerability inheritance and overtrust scenarios identified in AI-driven software engineering.

#### Overall Results Summary

| Model | Autonomous Failure Rate | Deception Rate | Recovery Success Rate | Constraint Adherence | Safety Status |
|-------|------------------------|----------------|----------------------|---------------------|---------------|
| **Stable Code 3B** | 25.0% | 22.6% | 76.0% | 87.6% | FAIL |
| **DeepSeek Coder 7B** | 29.6% | 19.2% | 77.0% | 87.6% | FAIL |
| **Yi-Coder 9B Chat** | 29.8% | 17.8% | 75.8% | 87.0% | FAIL |
| **Qwen2.5 Coder 7B** | 31.2% | 20.4% | 73.7% | 87.6% | FAIL |
| **CodeLlama 7B** | 31.4% | 19.4% | 73.2% | 85.0% | FAIL |
| **Granite 3B** | 34.0% | 17.8% | 66.5% | 85.6% | FAIL |

#### Critical Findings

1. **Universal Vulnerability Pattern**: All models showed systemic security weaknesses in generated code
2. **Complete Hallucination**: 100% hallucination rates across all models indicate severe reliability issues in generated code
3. **Autonomous Failure Crisis**: Failure rates ranging from 25.0% to 34.0% highlight the risks of unguarded autonomous AI agents
4. **Deception Resistance Variance**: Significant differences in deception rates (17.8% to 22.6%) suggest varying truthfulness capabilities
5. **Recovery Capability Gaps**: Recovery success rates between 66.5% and 77.0% indicate limited self-correction abilities

### Model-Specific Analysis

#### Stable Code 3B (Best Autonomous Performance)

- **Strengths**: Lowest autonomous failure rate (25.0%), strong constraint adherence (87.6%)
- **Weaknesses**: Highest deception rate (22.6%)
- **Risk Profile**: Demonstrates best autonomous behavior but highest deception tendency
- **Safety Implications**: Requires enhanced truthfulness validation for production use

#### Yi-Coder 9B Chat (Unique Security Profile)

- **Strengths**: Tied for lowest deception rate (17.8%), good recovery capabilities (75.8%)
- **Characteristics**: Balanced performance across safety metrics
- **Autonomous Performance**: Moderate failure rate (29.8%) with consistent constraint adherence
- **Security Focus**: Shows stable performance across multiple evaluation dimensions

#### DeepSeek Coder 7B (Best Recovery Capabilities)

- **Strengths**: Highest recovery success rate (77.0%), balanced performance across metrics
- **Characteristics**: Strong self-correction abilities with moderate autonomous failures (29.6%)
- **Model Architecture**: 7B parameters provide robust error recovery mechanisms

#### Granite 3B Code Instruct (Most Truthful)

- **Strengths**: Tied for lowest deception rate (17.8%), conservative approach to code generation
- **Weaknesses**: Highest autonomous failure rate (34.0%), lowest recovery success (66.5%)
- **Characteristics**: Prioritizes truthfulness over autonomous capability
- **Use Case**: Better suited for supervised rather than autonomous deployment

### Vulnerability Analysis

All models consistently generated code with:

1. **Command Injection Vulnerabilities**
   - `os.system()` usage without sanitization
   - `subprocess.run()` with shell=True
   - Missing input validation

2. **Common Weakness Enumerations (CWEs)**
   - CWE-78: OS Command Injection (primary finding)
   - High-risk patterns consistently detected

3. **Hallucination Patterns**
   - Fabricated method names
   - Non-existent API parameters
   - Missing error handling constructs

## Installation and Setup

### Prerequisites

```bash
# Python environment (3.10 recommended)
conda create -n reauto python=3.10
conda activate reauto

# GPU support (optional but recommended)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Dependencies

```bash
cd /absolute/path/to/aiSoftwarenegineering/modelEvaluation
pip install -r requirements.txt

# For dataset access (optional)
pip install datasets huggingface_hub
```

### Dataset Setup

The evaluation uses the Re-Auto-30K dataset, which can be accessed in two ways:

#### Option 1: Use Local CSV (Default)
The dataset is included locally in `datasets/prompts.csv` for immediate use.

#### Option 2: Download from Hugging Face
```python
from datasets import load_dataset

# Load the complete Re-Auto-30K dataset
dataset = load_dataset("navneetsatyamkumar/Re-Auto-30K")
```

### Model Setup

Models are stored in `/absolute/path/to/aiSoftwarenegineering/models/` and loaded automatically during evaluation.

## Usage

### Basic Evaluation

```bash
cd /absolute/path/to/aiSoftwarenegineering/modelEvaluation

# Interactive mode
python model-evaluation.py

# Background mode (recommended for full evaluation)
nohup python model-evaluation.py nohup > evaluation.log 2>&1 &
```

### Model Comparison

```bash
# Run comparison analysis
python run_comparison.py

# Manual comparison with specific data
python comapre-models.py
```

### Monitoring Progress

```bash
# Monitor nohup execution
tail -f evaluation.log

# Check specific model progress
tail -f output/run_*/model_name/combined_summary.md
```

## Output Documentation

### Directory Structure

```
output/
â”œâ”€â”€ run_20250813_215427_complete_nohup/    # Main evaluation results
â”‚   â”œâ”€â”€ multi_model_summary.csv           # Aggregated results
â”‚   â”œâ”€â”€ stable-code-3b/                   # Model-specific results
â”‚   â”‚   â”œâ”€â”€ vulnerability_metrics.json    # Detailed vulnerability data
â”‚   â”‚   â”œâ”€â”€ vulnerability_results.csv     # Vulnerability findings
â”‚   â”‚   â”œâ”€â”€ autonomous_metrics.json       # Autonomous behavior data
â”‚   â”‚   â”œâ”€â”€ autonomous_results.csv        # Autonomous test results
â”‚   â”‚   â”œâ”€â”€ combined_summary.md          # Executive summary
â”‚   â”‚   â””â”€â”€ *.png                        # Visualization charts
â”‚   â””â”€â”€ [other models follow same structure]
â””â”€â”€ modelsComparison/                     # Comparative analysis
    â””â”€â”€ comparison_run_20250814_091921/   # Timestamped comparison
        â”œâ”€â”€ model_metrics_heatmap.png     # Performance heatmap
        â”œâ”€â”€ grouped_metrics_bar_chart.png # Grouped comparison
        â”œâ”€â”€ individual_metric_charts.png  # Per-metric analysis
        â”œâ”€â”€ cleaned_metrics_data.csv      # Processed data
        â””â”€â”€ comparison_summary.txt        # Analysis report
```

### Key Output Files

1. **`multi_model_summary.csv`**: Aggregated metrics across all models
2. **`combined_summary.md`**: Executive summary for each model
3. **`vulnerability_metrics.json`**: Detailed vulnerability analysis
4. **`autonomous_metrics.json`**: Autonomous behavior test results
5. **Visualization Charts**: Professional charts with model logos

### Visualization Outputs

The framework generates professional visualizations including:

- **Heatmaps**: Model performance across all metrics
- **Bar Charts**: Individual metric comparisons with model logos
- **Line Charts**: Performance trends and patterns
- **Scatter Plots**: Correlation analysis between metrics

## Key Findings

### 1. Systemic Security Vulnerabilities

Our evaluation reveals critical patterns consistent with vulnerability inheritance in LLM-assisted code generation:

- **Universal Vulnerability**: All models showed significant vulnerability patterns in generated code
- **Hallucination Crisis**: 100% hallucination rates across all models indicate severe reliability issues
- **Security Gaps**: Models generated code with various security flaws requiring careful review
- **Pattern Consistency**: Similar vulnerability types across different architectures suggest training data issues

### 2. Autonomous Failure Ranking (Best to Worst)

**Performance Ranking by Autonomous Failure Rate:**

1. **Stable Code 3B**: 25.0% (Best autonomous performance)
2. **DeepSeek Coder 7B**: 29.6%
3. **Yi-Coder 9B Chat**: 29.8%
4. **Qwen2.5 Coder 7B**: 31.2%
5. **CodeLlama 7B**: 31.4%
6. **Granite 3B**: 34.0% (Most conservative, highest failure rate)

### 3. Safety Trade-offs and Risk Patterns

- **Truthfulness vs. Capability**: Models with lower deception rates often showed higher autonomous failure rates
- **Recovery vs. Prevention**: Better recovery capabilities didn't correlate with lower initial failure rates
- **Size vs. Safety**: Larger models (9B parameters) didn't consistently outperform smaller models (3B) in safety metrics
- **Performance Variability**: Different models excel in different safety dimensions, suggesting specialized use cases

### 4. Implications for AI-Driven Software Engineering

- **Overtrust Risk**: High failure rates combined with sophisticated outputs create dangerous overtrust scenarios
- **Governance Necessity**: All models require comprehensive safety frameworks before production deployment
- **Transparency Gaps**: High hallucination rates indicate fundamental issues with AI explainability
- **Regulatory Alignment**: Results support the need for frameworks like the SAFE-AI approach and compliance with emerging regulations

## Technical Implementation Details

### Multi-GPU Support

The framework supports distributed evaluation across multiple GPUs:

```python
# GPU assignment strategy
NUM_GPUS = 10  # RTX 3080 Ti configuration
# Sequential model loading with memory cleanup
# Ray-based distributed processing
```

### Memory Management

Robust memory management prevents OOM errors:

```python
# Automatic cleanup between models
def cleanup_model_from_gpu(model_name: str):
    # GPU memory cleanup
    # Model unloading
    # Ray actor cleanup
```

### Vulnerability Detection

Multi-layered security analysis:

```python
class VulnerabilityDetector:
    # Bandit static analysis
    # Custom pattern matching
    # CWE classification
    # Severity scoring
```

## Configuration

### Model Paths Configuration

```python
# In model-evaluation.py
MODEL_PATHS = {
    'stable-code-3b': '/absolute/path/to/models/stable-code-3b',
    'granite-3b-code-instruct-2k': '/absolute/path/to/models/granite-3b-code-instruct-2k',
    'deepseek-coder-7b-base-v1.5': '/absolute/path/to/models/deepseek-coder-7b-base-v1.5',
    'CodeLlama-7b-hf': '/absolute/path/to/models/CodeLlama-7b-hf',
    'Qwen2.5-Coder-7B-Instruct': '/absolute/path/to/models/Qwen2.5-Coder-7B-Instruct',
    'Yi-Coder-9B-Chat': '/absolute/path/to/models/Yi-Coder-9B-Chat'
}
```

### Output Configuration

```python
# Output directory structure
OUTPUT_ROOT = '/absolute/path/to/modelEvaluation/output'
# Timestamped run directories
# Model-specific subdirectories
```

## Future Work

### Planned Improvements

1. **Enhanced Security Analysis**
   - Integration with additional security scanners
   - Dynamic analysis capabilities
   - Runtime vulnerability detection

2. **Extended Model Support**
   - Larger models (13B, 34B parameters)
   - Domain-specific models
   - Fine-tuned security models

3. **Advanced Metrics**
   - Code quality metrics
   - Performance benchmarks
   - Maintainability scores

4. **Real-world Testing**
   - Integration with CI/CD pipelines
   - Production code analysis
   - Human expert validation

### Research Directions

1. **Vulnerability Mitigation**
   - Post-processing security filters
   - Training data improvements
   - Reinforcement learning from security feedback

2. **Autonomous Safety**
   - Better constraint enforcement
   - Improved self-correction mechanisms
   - Enhanced failure detection

3. **Benchmark Development**
   - Standardized security evaluation protocols
   - Industry-standard metrics
   - Comparative evaluation frameworks

## Contributing

### Dataset Contribution

This project includes our contribution to the open-source AI safety research community:

**ðŸ¤— Re-Auto-30K Dataset**: [navneetsatyamkumar/Re-Auto-30K](https://huggingface.co/datasets/navneetsatyamkumar/Re-Auto-30K)

This dataset contains over 30,000 carefully curated security-focused prompts designed for evaluating AI safety in code generation. The dataset is freely available for researchers and practitioners working on:

- AI safety evaluation frameworks
- LLM security assessment
- Autonomous agent safety research
- Code generation vulnerability analysis
- Benchmark development for AI safety

#### Dataset Features
- **30,886 unique prompts** covering diverse security scenarios
- **Categorized by security domain** for targeted evaluation
- **Complexity levels** ranging from basic to advanced security implementations
- **Multi-language support** including Python, JavaScript, Go, Rust, and more
- **Industry-relevant scenarios** based on real-world security requirements

#### Usage and Citation
If you use this dataset in your research, please cite our work and link to the Hugging Face dataset repository.

### Framework Contributions

This project serves as a foundation for AI safety research in code generation. Contributions welcome in:

- Additional security scanners and detection methods
- New evaluation metrics and safety protocols
- Extended model support and evaluation frameworks
- Visualization improvements and reporting tools
- Documentation and tutorial enhancements

## License and Citation

This research project evaluates publicly available models for academic and safety research purposes. Individual models are subject to their respective licenses.

---

